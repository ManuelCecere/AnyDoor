PeftModel(
  (base_model): LoraModel(
    (model): ControlLDM(
      (model): DiffusionWrapper(
        (diffusion_model): ControlledUnetModel(
          (time_embed): Sequential(...)
          (input_blocks): ModuleList(...)
          (middle_block): TimestepEmbedSequential(...)
          (output_blocks): ModuleList(
            (0-1): 2 x TimestepEmbedSequential(
              (0): ResBlock(
                (in_layers): Sequential(
                  (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (h_upd): Identity()
                (x_upd): Identity()
                (emb_layers): Sequential(
                  (0): SiLU()
                  (1): Linear(in_features=1280, out_features=1280, bias=True)
                )
                (out_layers): Sequential(
                  (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Dropout(p=0, inplace=False)
                  (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
              )
            )
            (2): TimestepEmbedSequential(
              (0): ResBlock(
                (in_layers): Sequential(
                  (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (h_upd): Identity()
                (x_upd): Identity()
                (emb_layers): Sequential(
                  (0): SiLU()
                  (1): Linear(in_features=1280, out_features=1280, bias=True)
                )
                (out_layers): Sequential(
                  (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Dropout(p=0, inplace=False)
                  (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
              )
              (1): Upsample(
                (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
            )
            (3-4): 2 x TimestepEmbedSequential(
              (0): ResBlock(
                (in_layers): Sequential(
                  (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (h_upd): Identity()
                (x_upd): Identity()
                (emb_layers): Sequential(
                  (0): SiLU()
                  (1): Linear(in_features=1280, out_features=1280, bias=True)
                )
                (out_layers): Sequential(
                  (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Dropout(p=0, inplace=False)
                  (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
              )
              (1): SpatialTransformer(
                (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
                (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
                (transformer_blocks): ModuleList(
                  (0): BasicTransformerBlock(
                    (attn1): MemoryEfficientCrossAttention(
                      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                      (to_out): Sequential(
                        (0): Linear(in_features=1280, out_features=1280, bias=True)
                        (1): Dropout(p=0.0, inplace=False)
                      )
                    )
                    (ff): FeedForward(
                      (net): Sequential(
                        (0): GEGLU(
                          (proj): Linear(in_features=1280, out_features=10240, bias=True)
                        )
                        (1): Dropout(p=0.0, inplace=False)
                        (2): Linear(in_features=5120, out_features=1280, bias=True)
                      )
                    )
                    (attn2): MemoryEfficientCrossAttention(
                      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                      (to_k): Linear(in_features=1024, out_features=1280, bias=False)
                      (to_v): Linear(in_features=1024, out_features=1280, bias=False)
                      (to_out): Sequential(
                        (0): Linear(in_features=1280, out_features=1280, bias=True)
                        (1): Dropout(p=0.0, inplace=False)
                      )
                    )
                    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                  )
                )
                (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
              )
            )
            (5): TimestepEmbedSequential(
              (0): ResBlock(
                (in_layers): Sequential(
                  (0): GroupNorm32(32, 1920, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (h_upd): Identity()
                (x_upd): Identity()
                (emb_layers): Sequential(
                  (0): SiLU()
                  (1): Linear(in_features=1280, out_features=1280, bias=True)
                )
                (out_layers): Sequential(
                  (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Dropout(p=0, inplace=False)
                  (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (skip_connection): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))
              )
              (1): SpatialTransformer(
                (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
                (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
                (transformer_blocks): ModuleList(
                  (0): BasicTransformerBlock(
                    (attn1): MemoryEfficientCrossAttention(
                      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                      (to_out): Sequential(
                        (0): Linear(in_features=1280, out_features=1280, bias=True)
                        (1): Dropout(p=0.0, inplace=False)
                      )
                    )
                    (ff): FeedForward(
                      (net): Sequential(
                        (0): GEGLU(
                          (proj): Linear(in_features=1280, out_features=10240, bias=True)
                        )
                        (1): Dropout(p=0.0, inplace=False)
                        (2): Linear(in_features=5120, out_features=1280, bias=True)
                      )
                    )
                    (attn2): MemoryEfficientCrossAttention(
                      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                      (to_k): Linear(in_features=1024, out_features=1280, bias=False)
                      (to_v): Linear(in_features=1024, out_features=1280, bias=False)
                      (to_out): Sequential(
                        (0): Linear(in_features=1280, out_features=1280, bias=True)
                        (1): Dropout(p=0.0, inplace=False)
                      )
                    )
                    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                  )
                )
                (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
              )
              (2): Upsample(
                (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
            )
            (6): TimestepEmbedSequential(
              (0): ResBlock(
                (in_layers): Sequential(
                  (0): GroupNorm32(32, 1920, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (h_upd): Identity()
                (x_upd): Identity()
                (emb_layers): Sequential(
                  (0): SiLU()
                  (1): Linear(in_features=1280, out_features=640, bias=True)
                )
                (out_layers): Sequential(
                  (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Dropout(p=0, inplace=False)
                  (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (skip_connection): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
              )
              (1): SpatialTransformer(
                (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
                (proj_in): Linear(in_features=640, out_features=640, bias=True)
                (transformer_blocks): ModuleList(
                  (0): BasicTransformerBlock(
                    (attn1): MemoryEfficientCrossAttention(
                      (to_q): Linear(in_features=640, out_features=640, bias=False)
                      (to_k): Linear(in_features=640, out_features=640, bias=False)
                      (to_v): Linear(in_features=640, out_features=640, bias=False)
                      (to_out): Sequential(
                        (0): Linear(in_features=640, out_features=640, bias=True)
                        (1): Dropout(p=0.0, inplace=False)
                      )
                    )
                    (ff): FeedForward(
                      (net): Sequential(
                        (0): GEGLU(
                          (proj): Linear(in_features=640, out_features=5120, bias=True)
                        )
                        (1): Dropout(p=0.0, inplace=False)
                        (2): Linear(in_features=2560, out_features=640, bias=True)
                      )
                    )
                    (attn2): MemoryEfficientCrossAttention(
                      (to_q): Linear(in_features=640, out_features=640, bias=False)
                      (to_k): Linear(in_features=1024, out_features=640, bias=False)
                      (to_v): Linear(in_features=1024, out_features=640, bias=False)
                      (to_out): Sequential(
                        (0): Linear(in_features=640, out_features=640, bias=True)
                        (1): Dropout(p=0.0, inplace=False)
                      )
                    )
                    (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                    (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                    (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                  )
                )
                (proj_out): Linear(in_features=640, out_features=640, bias=True)
              )
            )
            (7): TimestepEmbedSequential(
              (0): ResBlock(
                (in_layers): Sequential(
                  (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (h_upd): Identity()
                (x_upd): Identity()
                (emb_layers): Sequential(
                  (0): SiLU()
                  (1): Linear(in_features=1280, out_features=640, bias=True)
                )
                (out_layers): Sequential(
                  (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Dropout(p=0, inplace=False)
                  (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (skip_connection): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))
              )
              (1): SpatialTransformer(
                (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
                (proj_in): Linear(in_features=640, out_features=640, bias=True)
                (transformer_blocks): ModuleList(
                  (0): BasicTransformerBlock(
                    (attn1): MemoryEfficientCrossAttention(
                      (to_q): Linear(in_features=640, out_features=640, bias=False)
                      (to_k): Linear(in_features=640, out_features=640, bias=False)
                      (to_v): Linear(in_features=640, out_features=640, bias=False)
                      (to_out): Sequential(
                        (0): Linear(in_features=640, out_features=640, bias=True)
                        (1): Dropout(p=0.0, inplace=False)
                      )
                    )
                    (ff): FeedForward(
                      (net): Sequential(
                        (0): GEGLU(
                          (proj): Linear(in_features=640, out_features=5120, bias=True)
                        )
                        (1): Dropout(p=0.0, inplace=False)
                        (2): Linear(in_features=2560, out_features=640, bias=True)
                      )
                    )
                    (attn2): MemoryEfficientCrossAttention(
                      (to_q): Linear(in_features=640, out_features=640, bias=False)
                      (to_k): Linear(in_features=1024, out_features=640, bias=False)
                      (to_v): Linear(in_features=1024, out_features=640, bias=False)
                      (to_out): Sequential(
                        (0): Linear(in_features=640, out_features=640, bias=True)
                        (1): Dropout(p=0.0, inplace=False)
                      )
                    )
                    (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                    (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                    (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                  )
                )
                (proj_out): Linear(in_features=640, out_features=640, bias=True)
              )
            )
            (8): TimestepEmbedSequential(
              (0): ResBlock(
                (in_layers): Sequential(
                  (0): GroupNorm32(32, 960, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (h_upd): Identity()
                (x_upd): Identity()
                (emb_layers): Sequential(
                  (0): SiLU()
                  (1): Linear(in_features=1280, out_features=640, bias=True)
                )
                (out_layers): Sequential(
                  (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Dropout(p=0, inplace=False)
                  (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (skip_connection): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))
              )
              (1): SpatialTransformer(
                (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
                (proj_in): Linear(in_features=640, out_features=640, bias=True)
                (transformer_blocks): ModuleList(
                  (0): BasicTransformerBlock(
                    (attn1): MemoryEfficientCrossAttention(
                      (to_q): Linear(in_features=640, out_features=640, bias=False)
                      (to_k): Linear(in_features=640, out_features=640, bias=False)
                      (to_v): Linear(in_features=640, out_features=640, bias=False)
                      (to_out): Sequential(
                        (0): Linear(in_features=640, out_features=640, bias=True)
                        (1): Dropout(p=0.0, inplace=False)
                      )
                    )
                    (ff): FeedForward(
                      (net): Sequential(
                        (0): GEGLU(
                          (proj): Linear(in_features=640, out_features=5120, bias=True)
                        )
                        (1): Dropout(p=0.0, inplace=False)
                        (2): Linear(in_features=2560, out_features=640, bias=True)
                      )
                    )
                    (attn2): MemoryEfficientCrossAttention(
                      (to_q): Linear(in_features=640, out_features=640, bias=False)
                      (to_k): Linear(in_features=1024, out_features=640, bias=False)
                      (to_v): Linear(in_features=1024, out_features=640, bias=False)
                      (to_out): Sequential(
                        (0): Linear(in_features=640, out_features=640, bias=True)
                        (1): Dropout(p=0.0, inplace=False)
                      )
                    )
                    (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                    (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                    (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                  )
                )
                (proj_out): Linear(in_features=640, out_features=640, bias=True)
              )
              (2): Upsample(
                (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
            )
            (9): TimestepEmbedSequential(
              (0): ResBlock(
                (in_layers): Sequential(
                  (0): GroupNorm32(32, 960, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (h_upd): Identity()
                (x_upd): Identity()
                (emb_layers): Sequential(
                  (0): SiLU()
                  (1): Linear(in_features=1280, out_features=320, bias=True)
                )
                (out_layers): Sequential(
                  (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Dropout(p=0, inplace=False)
                  (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (skip_connection): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))
              )
              (1): SpatialTransformer(
                (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
                (proj_in): Linear(in_features=320, out_features=320, bias=True)
                (transformer_blocks): ModuleList(
                  (0): BasicTransformerBlock(
                    (attn1): MemoryEfficientCrossAttention(
                      (to_q): Linear(in_features=320, out_features=320, bias=False)
                      (to_k): Linear(in_features=320, out_features=320, bias=False)
                      (to_v): Linear(in_features=320, out_features=320, bias=False)
                      (to_out): Sequential(
                        (0): Linear(in_features=320, out_features=320, bias=True)
                        (1): Dropout(p=0.0, inplace=False)
                      )
                    )
                    (ff): FeedForward(
                      (net): Sequential(
                        (0): GEGLU(
                          (proj): Linear(in_features=320, out_features=2560, bias=True)
                        )
                        (1): Dropout(p=0.0, inplace=False)
                        (2): Linear(in_features=1280, out_features=320, bias=True)
                      )
                    )
                    (attn2): MemoryEfficientCrossAttention(
                      (to_q): Linear(in_features=320, out_features=320, bias=False)
                      (to_k): Linear(in_features=1024, out_features=320, bias=False)
                      (to_v): Linear(in_features=1024, out_features=320, bias=False)
                      (to_out): Sequential(
                        (0): Linear(in_features=320, out_features=320, bias=True)
                        (1): Dropout(p=0.0, inplace=False)
                      )
                    )
                    (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                    (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                    (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                  )
                )
                (proj_out): Linear(in_features=320, out_features=320, bias=True)
              )
            )
            (10-11): 2 x TimestepEmbedSequential(
              (0): ResBlock(
                (in_layers): Sequential(
                  (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (h_upd): Identity()
                (x_upd): Identity()
                (emb_layers): Sequential(
                  (0): SiLU()
                  (1): Linear(in_features=1280, out_features=320, bias=True)
                )
                (out_layers): Sequential(
                  (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Dropout(p=0, inplace=False)
                  (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (skip_connection): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))
              )
              (1): SpatialTransformer(
                (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
                (proj_in): Linear(in_features=320, out_features=320, bias=True)
                (transformer_blocks): ModuleList(
                  (0): BasicTransformerBlock(
                    (attn1): MemoryEfficientCrossAttention(
                      (to_q): Linear(in_features=320, out_features=320, bias=False)
                      (to_k): Linear(in_features=320, out_features=320, bias=False)
                      (to_v): Linear(in_features=320, out_features=320, bias=False)
                      (to_out): Sequential(
                        (0): Linear(in_features=320, out_features=320, bias=True)
                        (1): Dropout(p=0.0, inplace=False)
                      )
                    )
                    (ff): FeedForward(
                      (net): Sequential(
                        (0): GEGLU(
                          (proj): Linear(in_features=320, out_features=2560, bias=True)
                        )
                        (1): Dropout(p=0.0, inplace=False)
                        (2): Linear(in_features=1280, out_features=320, bias=True)
                      )
                    )
                    (attn2): MemoryEfficientCrossAttention(
                      (to_q): Linear(in_features=320, out_features=320, bias=False)
                      (to_k): Linear(in_features=1024, out_features=320, bias=False)
                      (to_v): Linear(in_features=1024, out_features=320, bias=False)
                      (to_out): Sequential(
                        (0): Linear(in_features=320, out_features=320, bias=True)
                        (1): Dropout(p=0.0, inplace=False)
                      )
                    )
                    (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                    (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                    (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                  )
                )
                (proj_out): Linear(in_features=320, out_features=320, bias=True)
              )
            )
          )
          (out): Sequential(
            (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
            (1): SiLU()
            (2): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
      )
      (first_stage_model): AutoencoderKL(
        (encoder): Encoder(
          (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (down): ModuleList(
            (0): Module(
              (block): ModuleList(
                (0-1): 2 x ResnetBlock(
                  (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
                  (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
                  (dropout): Dropout(p=0.0, inplace=False)
                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
              )
              (attn): ModuleList()
              (downsample): Downsample(
                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))
              )
            )
            (1): Module(
              (block): ModuleList(
                (0): ResnetBlock(
                  (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
                  (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
                  (dropout): Dropout(p=0.0, inplace=False)
                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (nin_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
                )
                (1): ResnetBlock(
                  (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
                  (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
                  (dropout): Dropout(p=0.0, inplace=False)
                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
              )
              (attn): ModuleList()
              (downsample): Downsample(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
              )
            )
            (2): Module(
              (block): ModuleList(
                (0): ResnetBlock(
                  (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
                  (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
                  (dropout): Dropout(p=0.0, inplace=False)
                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (nin_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
                )
                (1): ResnetBlock(
                  (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
                  (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
                  (dropout): Dropout(p=0.0, inplace=False)
                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
              )
              (attn): ModuleList()
              (downsample): Downsample(
                (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))
              )
            )
            (3): Module(
              (block): ModuleList(
                (0-1): 2 x ResnetBlock(
                  (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
                  (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
                  (dropout): Dropout(p=0.0, inplace=False)
                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
              )
              (attn): ModuleList()
            )
          )
          (mid): Module(
            (block_1): ResnetBlock(
              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (attn_1): MemoryEfficientAttnBlock(
              (norm): GroupNorm(32, 512, eps=1e-06, affine=True)
              (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
              (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
              (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
              (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
            )
            (block_2): ResnetBlock(
              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)
          (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (decoder): Decoder(
          (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (mid): Module(
            (block_1): ResnetBlock(
              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (attn_1): MemoryEfficientAttnBlock(
              (norm): GroupNorm(32, 512, eps=1e-06, affine=True)
              (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
              (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
              (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
              (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
            )
            (block_2): ResnetBlock(
              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (up): ModuleList(
            (0): Module(
              (block): ModuleList(
                (0): ResnetBlock(
                  (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
                  (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
                  (dropout): Dropout(p=0.0, inplace=False)
                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (nin_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
                )
                (1-2): 2 x ResnetBlock(
                  (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
                  (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
                  (dropout): Dropout(p=0.0, inplace=False)
                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
              )
              (attn): ModuleList()
            )
            (1): Module(
              (block): ModuleList(
                (0): ResnetBlock(
                  (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
                  (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
                  (dropout): Dropout(p=0.0, inplace=False)
                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (nin_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
                )
                (1-2): 2 x ResnetBlock(
                  (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
                  (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
                  (dropout): Dropout(p=0.0, inplace=False)
                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
              )
              (attn): ModuleList()
              (upsample): Upsample(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
            )
            (2-3): 2 x Module(
              (block): ModuleList(
                (0-2): 3 x ResnetBlock(
                  (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
                  (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
                  (dropout): Dropout(p=0.0, inplace=False)
                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
              )
              (attn): ModuleList()
              (upsample): Upsample(
                (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
            )
          )
          (norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)
          (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (loss): Identity()
        (quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))
        (post_quant_conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))
      )
      (cond_stage_model): FrozenDinoV2Encoder(
        (model): DinoVisionTransformer(
          (patch_embed): PatchEmbed(
            (proj): Conv2d(3, 1536, kernel_size=(14, 14), stride=(14, 14))
            (norm): Identity()
          )
          (blocks): ModuleList(
            (0-39): 40 x NestedTensorBlock(
              (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
              (attn): MemEffAttention(
                (qkv): Linear(in_features=1536, out_features=4608, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=1536, out_features=1536, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): LayerScale()
              (drop_path1): Identity()
              (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
              (mlp): SwiGLUFFNFused(
                (w12): Linear(in_features=1536, out_features=8192, bias=True)
                (w3): Linear(in_features=4096, out_features=1536, bias=True)
              )
              (ls2): LayerScale()
              (drop_path2): Identity()
            )
          )
          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
          (head): Identity()
        )
        (projector): Linear(in_features=1536, out_features=1024, bias=True)
      )
      (control_model): ControlNet(
        (time_embed): Sequential(
          (0): Linear(in_features=320, out_features=1280, bias=True)
          (1): SiLU()
          (2): Linear(in_features=1280, out_features=1280, bias=True)
        )
        (input_blocks): ModuleList(
          (0): TimestepEmbedSequential(
            (0): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (1-2): 2 x TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=320, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Identity()
            )
            (1): SpatialTransformer(
              (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
              (proj_in): Linear(in_features=320, out_features=320, bias=True)
              (transformer_blocks): ModuleList(
                (0): BasicTransformerBlock(
                  (attn1): MemoryEfficientCrossAttention(
                    (to_q): Linear(in_features=320, out_features=320, bias=False)
                    (to_k): Linear(in_features=320, out_features=320, bias=False)
                    (to_v): Linear(in_features=320, out_features=320, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=320, out_features=320, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (ff): FeedForward(
                    (net): Sequential(
                      (0): GEGLU(
                        (proj): Linear(in_features=320, out_features=2560, bias=True)
                      )
                      (1): Dropout(p=0.0, inplace=False)
                      (2): Linear(in_features=1280, out_features=320, bias=True)
                    )
                  )
                  (attn2): MemoryEfficientCrossAttention(
                    (to_q): Linear(in_features=320, out_features=320, bias=False)
                    (to_k): Linear(in_features=1024, out_features=320, bias=False)
                    (to_v): Linear(in_features=1024, out_features=320, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=320, out_features=320, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                  (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                )
              )
              (proj_out): Linear(in_features=320, out_features=320, bias=True)
            )
          )
          (3): TimestepEmbedSequential(
            (0): Downsample(
              (op): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
            )
          )
          (4): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=640, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): SpatialTransformer(
              (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
              (proj_in): Linear(in_features=640, out_features=640, bias=True)
              (transformer_blocks): ModuleList(
                (0): BasicTransformerBlock(
                  (attn1): MemoryEfficientCrossAttention(
                    (to_q): Linear(in_features=640, out_features=640, bias=False)
                    (to_k): Linear(in_features=640, out_features=640, bias=False)
                    (to_v): Linear(in_features=640, out_features=640, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=640, out_features=640, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (ff): FeedForward(
                    (net): Sequential(
                      (0): GEGLU(
                        (proj): Linear(in_features=640, out_features=5120, bias=True)
                      )
                      (1): Dropout(p=0.0, inplace=False)
                      (2): Linear(in_features=2560, out_features=640, bias=True)
                    )
                  )
                  (attn2): MemoryEfficientCrossAttention(
                    (to_q): Linear(in_features=640, out_features=640, bias=False)
                    (to_k): Linear(in_features=1024, out_features=640, bias=False)
                    (to_v): Linear(in_features=1024, out_features=640, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=640, out_features=640, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                )
              )
              (proj_out): Linear(in_features=640, out_features=640, bias=True)
            )
          )
          (5): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=640, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Identity()
            )
            (1): SpatialTransformer(
              (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
              (proj_in): Linear(in_features=640, out_features=640, bias=True)
              (transformer_blocks): ModuleList(
                (0): BasicTransformerBlock(
                  (attn1): MemoryEfficientCrossAttention(
                    (to_q): Linear(in_features=640, out_features=640, bias=False)
                    (to_k): Linear(in_features=640, out_features=640, bias=False)
                    (to_v): Linear(in_features=640, out_features=640, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=640, out_features=640, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (ff): FeedForward(
                    (net): Sequential(
                      (0): GEGLU(
                        (proj): Linear(in_features=640, out_features=5120, bias=True)
                      )
                      (1): Dropout(p=0.0, inplace=False)
                      (2): Linear(in_features=2560, out_features=640, bias=True)
                    )
                  )
                  (attn2): MemoryEfficientCrossAttention(
                    (to_q): Linear(in_features=640, out_features=640, bias=False)
                    (to_k): Linear(in_features=1024, out_features=640, bias=False)
                    (to_v): Linear(in_features=1024, out_features=640, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=640, out_features=640, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                )
              )
              (proj_out): Linear(in_features=640, out_features=640, bias=True)
            )
          )
          (6): TimestepEmbedSequential(
            (0): Downsample(
              (op): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
            )
          )
          (7): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=1280, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): SpatialTransformer(
              (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
              (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
              (transformer_blocks): ModuleList(
                (0): BasicTransformerBlock(
                  (attn1): MemoryEfficientCrossAttention(
                    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=1280, out_features=1280, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (ff): FeedForward(
                    (net): Sequential(
                      (0): GEGLU(
                        (proj): Linear(in_features=1280, out_features=10240, bias=True)
                      )
                      (1): Dropout(p=0.0, inplace=False)
                      (2): Linear(in_features=5120, out_features=1280, bias=True)
                    )
                  )
                  (attn2): MemoryEfficientCrossAttention(
                    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_k): Linear(in_features=1024, out_features=1280, bias=False)
                    (to_v): Linear(in_features=1024, out_features=1280, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=1280, out_features=1280, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                )
              )
              (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
            )
          )
          (8): TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=1280, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Identity()
            )
            (1): SpatialTransformer(
              (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
              (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
              (transformer_blocks): ModuleList(
                (0): BasicTransformerBlock(
                  (attn1): MemoryEfficientCrossAttention(
                    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=1280, out_features=1280, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (ff): FeedForward(
                    (net): Sequential(
                      (0): GEGLU(
                        (proj): Linear(in_features=1280, out_features=10240, bias=True)
                      )
                      (1): Dropout(p=0.0, inplace=False)
                      (2): Linear(in_features=5120, out_features=1280, bias=True)
                    )
                  )
                  (attn2): MemoryEfficientCrossAttention(
                    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                    (to_k): Linear(in_features=1024, out_features=1280, bias=False)
                    (to_v): Linear(in_features=1024, out_features=1280, bias=False)
                    (to_out): Sequential(
                      (0): Linear(in_features=1280, out_features=1280, bias=True)
                      (1): Dropout(p=0.0, inplace=False)
                    )
                  )
                  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                )
              )
              (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
            )
          )
          (9): TimestepEmbedSequential(
            (0): Downsample(
              (op): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
            )
          )
          (10-11): 2 x TimestepEmbedSequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (emb_layers): Sequential(
                (0): SiLU()
                (1): Linear(in_features=1280, out_features=1280, bias=True)
              )
              (out_layers): Sequential(
                (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Identity()
            )
          )
        )
        (zero_convs): ModuleList(
          (0-3): 4 x TimestepEmbedSequential(
            (0): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (4-6): 3 x TimestepEmbedSequential(
            (0): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
          )
          (7-11): 5 x TimestepEmbedSequential(
            (0): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (input_hint_block): TimestepEmbedSequential(
          (0): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): SiLU()
          (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (3): SiLU()
          (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (5): SiLU()
          (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (7): SiLU()
          (8): Conv2d(32, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (9): SiLU()
          (10): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (11): SiLU()
          (12): Conv2d(96, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (13): SiLU()
          (14): Conv2d(256, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (middle_block): TimestepEmbedSequential(
          (0): ResBlock(
            (in_layers): Sequential(
              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (h_upd): Identity()
            (x_upd): Identity()
            (emb_layers): Sequential(
              (0): SiLU()
              (1): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (out_layers): Sequential(
              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0, inplace=False)
              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (skip_connection): Identity()
          )
          (1): SpatialTransformer(
            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
            (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (attn1): MemoryEfficientCrossAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=1280, out_features=10240, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=5120, out_features=1280, bias=True)
                  )
                )
                (attn2): MemoryEfficientCrossAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=1024, out_features=1280, bias=False)
                  (to_v): Linear(in_features=1024, out_features=1280, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
          )
          (2): ResBlock(
            (in_layers): Sequential(
              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (h_upd): Identity()
            (x_upd): Identity()
            (emb_layers): Sequential(
              (0): SiLU()
              (1): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (out_layers): Sequential(
              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
              (1): SiLU()
              (2): Dropout(p=0, inplace=False)
              (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (skip_connection): Identity()
          )
        )
        (middle_block_out): TimestepEmbedSequential(
          (0): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
        )
      )
    )
  )
)
