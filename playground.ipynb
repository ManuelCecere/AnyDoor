{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from dress_code_data import DressCodeDatasetAnyDoor\n",
    "# from utils import sem2onehot\n",
    "import argparse\n",
    "from matplotlib import pyplot as plt\n",
    "import PIL\n",
    "from torchvision.transforms import ToPILImage,Grayscale,ToTensor\n",
    "from datasets.vitonhd import VitonHDDataset, VitonHDDataset_agnostic\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler,Subset\n",
    "import cv2\n",
    "import random\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image, ImageDraw\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from typing import List, Tuple\n",
    "from dress_code_data.labelmap import label_map\n",
    "from numpy.linalg import lstsq\n",
    "from omegaconf import OmegaConf\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_mask_area(mask, max_ratio=0.7, min_ratio=0.1):\n",
    "    H, W = mask.shape[0], mask.shape[1]\n",
    "    ratio = mask.sum() / (H * W)\n",
    "    logger.info(f\"Dimensions:{mask.shape}\")\n",
    "    logger.info(f\"sum over the mask {mask.sum()}\")\n",
    "    logger.info(f\"Ratio computed: {ratio}\")\n",
    "    if (ratio > max_ratio) or (ratio < min_ratio):\n",
    "        logger.error(f\"ratio computed: {ratio}\")\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "    import numpy as np\n",
    "import torch\n",
    "\n",
    "def batch_check_mask_area(batch_masks, max_ratio=0.7, min_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Check if the area of each mask in a batch falls within the specified ratio bounds.\n",
    "\n",
    "    Parameters:\n",
    "    - batch_masks: A batch of masks with shape (N, H, W).\n",
    "    - max_ratio: Maximum allowed ratio of mask area to total area.\n",
    "    - min_ratio: Minimum allowed ratio of mask area to total area.\n",
    "\n",
    "    Returns:\n",
    "    - A list of boolean values indicating whether each mask meets the area criteria.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for mask in batch_masks:\n",
    "        H, W = mask.shape[0], mask.shape[1]  # Adjusted for batched masks\n",
    "        ratio = mask.sum() / (H * W)\n",
    "    \n",
    "        if (ratio > max_ratio) or (ratio < min_ratio):\n",
    "            logger.info(f\"Ratio computed: {ratio}\")\n",
    "            results.append(False)\n",
    "        else:\n",
    "            results.append(True)\n",
    "    \n",
    "    return np.array(results)\n",
    "\n",
    "def mask_score(mask):\n",
    "    \"\"\"Scoring the mask according to connectivity.\"\"\"\n",
    "    mask = mask.astype(np.uint8)\n",
    "    if mask.sum() < 10:\n",
    "        return 0\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    cnt_area = [cv2.contourArea(cnt) for cnt in contours]\n",
    "    conc_score = np.max(cnt_area) / sum(cnt_area)\n",
    "    if conc_score < 0.9:\n",
    "        logger.error(f\"mask score under the threshold: {conc_score}\")\n",
    "    return conc_score\n",
    "\n",
    "\n",
    "def batch_mask_score(batch_masks):\n",
    "    \"\"\"\n",
    "    Compute the mask score for a batch of masks.\n",
    "    \n",
    "    Parameters:\n",
    "    - batch_masks: A batch of masks as a numpy array or a PyTorch tensor.\n",
    "                   Shape should be (N, H, W) where N is the batch size.\n",
    "    \n",
    "    Returns:\n",
    "    - scores: A numpy array or PyTorch tensor of scores for each mask in the batch.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for mask in batch_masks:\n",
    "        mask = mask.numpy().astype(np.uint8) if isinstance(mask, torch.Tensor) else mask.astype(np.uint8)\n",
    "        if mask.sum() < 10:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "        cnt_area = [cv2.contourArea(cnt) for cnt in contours]\n",
    "        conc_score = np.max(cnt_area) / sum(cnt_area) if sum(cnt_area) > 0 else 0\n",
    "        if conc_score < 0.9:\n",
    "            # Assuming `logger` is defined elsewhere.\n",
    "            logger.info(f\"mask score under the threshold: {conc_score}\")\n",
    "        scores.append(conc_score)\n",
    "    \n",
    "    return np.array(scores)\n",
    "\n",
    "def check_mask_area(self, mask, max_ratio=0.7, min_ratio=0.01):\n",
    "        H, W = mask.shape[0], mask.shape[1]\n",
    "        ratio = mask.sum() / (H * W)\n",
    "        logger.info(f\"Dimensions:{mask.shape}\")\n",
    "        logger.info(f\"sum over the mask {mask.sum()}\")\n",
    "        logger.info(f\"Ratio computed: {ratio}\")\n",
    "        if (ratio > max_ratio) or (ratio < min_ratio):\n",
    "            logger.error(f\"ratio computed: {ratio}\")\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "        \n",
    "        \n",
    "def check_region_size(self, image, yyxx, ratio, mode=\"max\"):\n",
    "    pass_flag = True\n",
    "    H, W = image.shape[0], image.shape[1]\n",
    "    H, W = H * ratio, W * ratio\n",
    "    y1, y2, x1, x2 = yyxx\n",
    "    h, w = y2 - y1, x2 - x1\n",
    "    if mode == \"max\":\n",
    "        if h > H or w > W:\n",
    "            pass_flag = False\n",
    "    elif mode == \"min\":\n",
    "        if h < H or w < W:\n",
    "            pass_flag = False\n",
    "    return pass_flag\n",
    "    \n",
    "def batch_check_region_size(batch_images, batch_yyxx, ratio, mode=\"max\"):\n",
    "    \"\"\"\n",
    "    Check if the regions of each image in a batch are within specified size constraints.\n",
    "    \n",
    "    Parameters:\n",
    "    - batch_images: A batch of images with shape (N, H, W, C) or (N, H, W) where\n",
    "                    N is the batch size, H and W are the dimensions of the images,\n",
    "                    and C is the number of channels (if present).\n",
    "    - batch_yyxx: A list or array of region coordinates for each image in the batch,\n",
    "                  with each element being [y1, y2, x1, x2].\n",
    "    - ratio: A float representing the ratio of the region size to the image size.\n",
    "    - mode: A string, either \"max\" or \"min\", indicating the type of size check.\n",
    "    \n",
    "    Returns:\n",
    "    - A numpy array of boolean values indicating whether each region in the batch meets the size criteria.\n",
    "    \"\"\"\n",
    "    pass_flags = []\n",
    "    for image, yyxx in zip(batch_images, batch_yyxx):\n",
    "        H, W = image.shape[0], image.shape[1]\n",
    "        H, W = H * ratio, W * ratio\n",
    "        y1, y2, x1, x2 = yyxx\n",
    "        h, w = y2 - y1, x2 - x1\n",
    "        pass_flag = True\n",
    "        if mode == \"max\":\n",
    "            if h > H or w > W:\n",
    "                pass_flag = False\n",
    "        elif mode == \"min\":\n",
    "            if h < H or w < W:\n",
    "                pass_flag = False\n",
    "        pass_flags.append(pass_flag)\n",
    "    return np.array(pass_flags)\n",
    "\n",
    "\n",
    "def get_bbox_from_mask(mask):\n",
    "    h, w = mask.shape[0], mask.shape[1]\n",
    "\n",
    "    if mask.sum() < 10:\n",
    "        return 0, h, 0, w\n",
    "    rows = np.any(mask, axis=1)\n",
    "    cols = np.any(mask, axis=0)\n",
    "    y1, y2 = np.where(rows)[0][[0, -1]]\n",
    "    x1, x2 = np.where(cols)[0][[0, -1]]\n",
    "    return (y1, y2, x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_worker(args):\n",
    "\n",
    "    # Dataset & Dataloader\n",
    "    dataset_train = DressCodeDatasetAnyDoor(\n",
    "        args,\n",
    "        dataroot_path=args.dataroot,\n",
    "        phase=\"train\",\n",
    "        order=\"paired\",\n",
    "        size=(int(args.height), int(args.width)),\n",
    "    )\n",
    "    total_data = len(dataset_train)\n",
    "    subset_size = total_data // 1\n",
    "\n",
    "    # Generate a random sample of indexes for the subset\n",
    "    random_indexes = random.sample(range(total_data), subset_size)\n",
    "    \n",
    "    # indexes = list(range(44000, len(dataset_train)))\n",
    "    # subset_size = len(indexes)\n",
    "    \n",
    "    print(\"dataset size:\", subset_size)\n",
    "\n",
    "\n",
    "    # Creating a subset of the original dataset using the randomly selected indexes\n",
    "    random_subset_dataset_train = Subset(dataset_train, random_indexes)\n",
    "\n",
    "    dataloader_train = torch.utils.data.DataLoader(random_subset_dataset_train, batch_size=128, shuffle=False, num_workers=4)\n",
    "\n",
    "    return dataloader_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 46521\n"
     ]
    }
   ],
   "source": [
    "args = argparse.Namespace(batch_size=1, category='all', checkpoint_dir='', data_pairs='{}_pairs', dataroot='/home/ubuntu/volume240/DressCode', display_count=1000, epochs=150, exp_name='', height=1024, radius=5, shuffle=True, step=100000, width=768, workers=0)\n",
    "\n",
    "# Call main worker\n",
    "dataloader_train  = main_worker(args)\n",
    "sample_it  = iter(dataloader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batches(dataloader):\n",
    "    incorrect_masks_count = 0\n",
    "    total_batches = len(dataloader)\n",
    "    print(\"Processing batches...\")\n",
    "\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        # Assuming batch['ref_mask'] is a tensor here\n",
    "        masks = batch[\"ref_mask\"]\n",
    "        model_masks = batch[\"model_mask\"]\n",
    "        boxes = [get_bbox_from_mask(mask) for mask in np.array(batch[\"ref_mask\"])]\n",
    "        \n",
    "        unconnected_masks = batch_mask_score(masks) < 0.9\n",
    "        wrong_size_masks = ~batch_check_mask_area(masks)\n",
    "        wrong_size_model_masks = ~batch_check_mask_area(model_masks)\n",
    "        wrong_size_boxes = ~batch_check_region_size(batch[\"ref_image\"],batch_yyxx = boxes, ratio = 0.1 , mode = \"min\")\n",
    "        \n",
    "        # print(f\"unconnected_masks sizes: {unconnected_masks.shape}, wrong_size_masks sizes: {wrong_size_masks.shape}\", )\n",
    "        incorrect_masks_count += np.sum( wrong_size_boxes )\n",
    "        \n",
    "        # Progress update\n",
    "        progress = (batch_idx + 1) / total_batches * 100\n",
    "        print(f'\\r[{(\"=\" * int(progress // 2)) + (\" \" * (50 - int(progress // 2)))}] {progress:.2f}%', end='')\n",
    "\n",
    "    print(\"\\nFinished processing. Checks not passed: \",incorrect_masks_count )\n",
    "    \n",
    "    return incorrect_masks_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/mnt/myData/AnyDoor/.venv/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/home/ubuntu/mnt/myData/AnyDoor/.venv/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/home/ubuntu/mnt/myData/AnyDoor/.venv/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/home/ubuntu/mnt/myData/AnyDoor/.venv/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/mnt/myData/AnyDoor/.venv/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/home/ubuntu/mnt/myData/AnyDoor/.venv/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/home/ubuntu/mnt/myData/AnyDoor/.venv/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/home/ubuntu/mnt/myData/AnyDoor/.venv/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.00%\n",
      "Finished processing. Checks not passed:  15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_batches(dataloader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(sample_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ref_image', 'ref_mask', 'model_image', 'model_mask', 'c_name', 'im_name', 'dataroot'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(29, 1001, 83, 685),\n",
       " (89, 938, 230, 529),\n",
       " (89, 939, 265, 506),\n",
       " (270, 758, 183, 590),\n",
       " (151, 935, 216, 553),\n",
       " (89, 940, 279, 493),\n",
       " (95, 940, 238, 515),\n",
       " (154, 867, 105, 660),\n",
       " (89, 939, 212, 549),\n",
       " (86, 940, 191, 581),\n",
       " (89, 938, 222, 550),\n",
       " (108, 952, 248, 527),\n",
       " (255, 768, 69, 702),\n",
       " (185, 857, 58, 713),\n",
       " (181, 939, 194, 579),\n",
       " (90, 937, 94, 677),\n",
       " (63, 968, 190, 589),\n",
       " (91, 940, 243, 530),\n",
       " (219, 805, 72, 696),\n",
       " (93, 937, 143, 606),\n",
       " (88, 939, 224, 547),\n",
       " (58, 973, 185, 603),\n",
       " (70, 941, 228, 578),\n",
       " (83, 650, 157, 625),\n",
       " (89, 935, 176, 597),\n",
       " (262, 768, 75, 698),\n",
       " (89, 940, 129, 641),\n",
       " (90, 938, 256, 504),\n",
       " (91, 935, 239, 527),\n",
       " (160, 870, 62, 709),\n",
       " (165, 936, 228, 544),\n",
       " (79, 947, 221, 550),\n",
       " (235, 832, 120, 656),\n",
       " (86, 940, 166, 606),\n",
       " (87, 941, 143, 627),\n",
       " (88, 943, 187, 586),\n",
       " (93, 933, 241, 531),\n",
       " (86, 940, 180, 593),\n",
       " (98, 940, 245, 524),\n",
       " (66, 970, 117, 650),\n",
       " (89, 937, 236, 535),\n",
       " (68, 965, 207, 565),\n",
       " (0, 959, 0, 767),\n",
       " (116, 902, 13, 749),\n",
       " (91, 936, 164, 607),\n",
       " (73, 958, 150, 619),\n",
       " (91, 900, 191, 550),\n",
       " (90, 939, 159, 604),\n",
       " (88, 933, 151, 615),\n",
       " (181, 853, 113, 664),\n",
       " (90, 945, 200, 572),\n",
       " (92, 939, 161, 589),\n",
       " (251, 776, 122, 623),\n",
       " (90, 937, 197, 576),\n",
       " (93, 936, 186, 593),\n",
       " (87, 940, 137, 633),\n",
       " (230, 850, 122, 659),\n",
       " (125, 890, 71, 702),\n",
       " (94, 935, 154, 618),\n",
       " (86, 938, 146, 625),\n",
       " (84, 941, 232, 539),\n",
       " (0, 1023, 0, 767),\n",
       " (90, 940, 198, 572),\n",
       " (133, 940, 224, 549),\n",
       " (90, 938, 211, 560),\n",
       " (92, 934, 240, 531),\n",
       " (78, 958, 185, 590),\n",
       " (90, 938, 218, 553),\n",
       " (218, 812, 103, 668),\n",
       " (86, 940, 145, 626),\n",
       " (0, 1023, 172, 767),\n",
       " (92, 936, 151, 619),\n",
       " (121, 903, 171, 600),\n",
       " (95, 938, 246, 526),\n",
       " (199, 823, 65, 704),\n",
       " (92, 929, 203, 565),\n",
       " (74, 983, 169, 606),\n",
       " (221, 823, 117, 656),\n",
       " (111, 938, 270, 496),\n",
       " (87, 939, 204, 567),\n",
       " (119, 908, 0, 757),\n",
       " (88, 936, 148, 624),\n",
       " (78, 955, 217, 558),\n",
       " (87, 941, 153, 618),\n",
       " (131, 898, 65, 707),\n",
       " (67, 970, 187, 606),\n",
       " (99, 933, 158, 609),\n",
       " (85, 941, 240, 533),\n",
       " (80, 947, 131, 651),\n",
       " (163, 780, 174, 596),\n",
       " (114, 908, 228, 540),\n",
       " (0, 1023, 0, 767),\n",
       " (74, 946, 88, 683),\n",
       " (92, 959, 119, 666),\n",
       " (82, 948, 118, 659),\n",
       " (102, 937, 188, 586),\n",
       " (88, 937, 173, 598),\n",
       " (88, 932, 142, 621),\n",
       " (91, 939, 224, 550),\n",
       " (88, 940, 190, 582),\n",
       " (173, 820, 118, 670),\n",
       " (90, 920, 151, 622),\n",
       " (93, 931, 257, 513),\n",
       " (104, 923, 131, 623),\n",
       " (86, 942, 254, 519),\n",
       " (141, 885, 115, 655),\n",
       " (87, 938, 145, 623),\n",
       " (124, 933, 243, 529),\n",
       " (91, 939, 219, 549),\n",
       " (86, 940, 166, 602),\n",
       " (87, 939, 144, 627),\n",
       " (88, 945, 157, 614),\n",
       " (88, 938, 228, 544),\n",
       " (95, 937, 261, 520),\n",
       " (90, 928, 219, 552),\n",
       " (140, 885, 89, 678),\n",
       " (72, 963, 187, 592),\n",
       " (179, 774, 158, 600),\n",
       " (87, 940, 149, 621),\n",
       " (93, 767, 157, 626),\n",
       " (93, 937, 290, 482),\n",
       " (93, 939, 164, 605),\n",
       " (144, 899, 65, 707),\n",
       " (57, 956, 68, 703),\n",
       " (72, 956, 152, 625),\n",
       " (164, 939, 220, 549),\n",
       " (249, 775, 184, 624),\n",
       " (92, 940, 200, 571)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[get_bbox_from_mask(mask) for mask in np.array(sample[\"ref_mask\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set([c.split(\"/\")[-1] for c in sample[\"dataroot\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_pairs_file(dataloader):\n",
    "    \"\"\" Filters out the items not passing the mask checks and write the correct ones in a txt file used for dataset indexing\n",
    "        The incorrect_masks_count is returned as double check\"\"\"\n",
    "    incorrect_masks_count = 0\n",
    "    total_batches = len(dataloader)\n",
    "    print(\"Processing batches...\")\n",
    "    with open(\"train_pairs_filtered.txt\", \"a\") as file:\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            # Assuming batch['-'] is a tensor here\n",
    "            masks = batch[\"ref_mask\"]\n",
    "            model_masks = batch[\"model_mask\"]\n",
    "            c_names = batch[\"c_name\"]\n",
    "            im_names = batch[\"im_name\"]\n",
    "            categories = [c.split(\"/\")[-1] for c in batch[\"dataroot\"]]\n",
    "            \n",
    "            \n",
    "            #we check for connectivity score and size of the masks\n",
    "            unconnected_masks = batch_mask_score(masks) < 0.9\n",
    "            wrong_size_masks = ~batch_check_mask_area(masks)\n",
    "            wrong_size_model_masks = ~batch_check_mask_area(model_masks)\n",
    "            \n",
    "            \n",
    "            items_filtered = ~(unconnected_masks| wrong_size_masks|wrong_size_model_masks)\n",
    "            incorrect_masks_count += np.sum(~items_filtered)\n",
    "            \n",
    "            #Filter the names of pairs that pass the mask checks\n",
    "            print()\n",
    "            c_names_filtered = np.array(c_names)[items_filtered]\n",
    "            im_names_filtered = np.array(im_names)[items_filtered]\n",
    "            categories_filtered= np.array(categories)[items_filtered]\n",
    "            \n",
    "            # Write filtered names line by line\n",
    "            for c_name, im_name in zip(c_names_filtered, im_names_filtered):\n",
    "                file.write(f\"{im_name} {c_name}\\n\")\n",
    "            \n",
    "            # Progress update\n",
    "            progress = (batch_idx + 1) / total_batches * 100\n",
    "            print(f'\\r[{(\"=\" * int(progress // 2)) + (\" \" * (50 - int(progress // 2)))}] {progress:.2f}%', end='')\n",
    "\n",
    "    print(\"\\nFinished processing. Checks not passed: \",incorrect_masks_count )\n",
    "    \n",
    "    return incorrect_masks_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_pairs_file_for_category(dataloader):\n",
    "    \"\"\"Filters out the items not passing the mask checks and write the correct ones in different txt files used for dataset indexing based on their category.\n",
    "    The incorrect_masks_count is returned as a double check.\"\"\"\n",
    "    incorrect_masks_count = 0\n",
    "    total_batches = len(dataloader)\n",
    "    print(\"Processing batches...\")\n",
    "\n",
    "    # Open files for each category in append mode\n",
    "    files = {\n",
    "        'dresses': open(\"train_pairs_filtered_dresses.txt\", \"a\"),\n",
    "        'lower_body': open(\"train_pairs_filtered_lower_body.txt\", \"a\"),\n",
    "        'upper_body': open(\"train_pairs_filtered_upper_body.txt\", \"a\")\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            masks = batch[\"ref_mask\"]\n",
    "            model_masks = batch[\"model_mask\"]\n",
    "            c_names = batch[\"c_name\"]\n",
    "            im_names = batch[\"im_name\"]\n",
    "            categories = [c.split(\"/\")[-1] for c in batch[\"dataroot\"]]\n",
    "            \n",
    "            # Check for connectivity score and size of the masks\n",
    "            unconnected_masks = batch_mask_score(masks) < 0.9\n",
    "            wrong_size_masks = ~batch_check_mask_area(masks)\n",
    "            wrong_size_model_masks = ~batch_check_mask_area(model_masks)\n",
    "            \n",
    "            items_filtered = ~(unconnected_masks | wrong_size_masks | wrong_size_model_masks)\n",
    "            incorrect_masks_count += np.sum(~items_filtered)\n",
    "            \n",
    "            # Convert to arrays for boolean indexing\n",
    "            c_names_filtered = np.array(c_names)[items_filtered]\n",
    "            im_names_filtered = np.array(im_names)[items_filtered]\n",
    "            categories_filtered = np.array(categories)[items_filtered]\n",
    "            \n",
    "            # Write filtered names line by line to corresponding category file\n",
    "            for category, im_name, c_name in zip(categories_filtered, im_names_filtered, c_names_filtered):\n",
    "                if category in files:\n",
    "                    files[category].write(f\"{im_name} {c_name}\\n\")\n",
    "            \n",
    "            # Progress update\n",
    "            progress = (batch_idx + 1) / total_batches * 100\n",
    "            print(f'\\r[{(\"=\" * int(progress // 2)) + (\" \" * (50 - int(progress // 2)))}] {progress:.2f}%', end='')\n",
    "    finally:\n",
    "        # Close all files\n",
    "        for file in files.values():\n",
    "            file.close()\n",
    "\n",
    "    print(\"\\nFinished processing. Checks not passed: \", incorrect_masks_count)\n",
    "    \n",
    "    return incorrect_masks_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/mnt/myvolume/AnyDoor/.venv/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/home/ubuntu/mnt/myvolume/AnyDoor/.venv/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/home/ubuntu/mnt/myvolume/AnyDoor/.venv/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/home/ubuntu/mnt/myvolume/AnyDoor/.venv/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.00%\n",
      "Finished processing. Checks not passed:  1871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1871"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remember to delete the txt files before this cell, or it will append the lines\n",
    "write_pairs_file_for_category(dataloader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DConf = OmegaConf.load(\"./configs/datasets.yaml\")\n",
    "viton_dataset_train = VitonHDDataset_agnostic(**DConf.Train.VitonHD)\n",
    "dataloader_train = torch.utils.data.DataLoader(\n",
    "    viton_dataset_train,\n",
    "    num_workers=8,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    )\n",
    "\n",
    "sample_viton_iter = iter(dataloader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_viton = next(sample_viton_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_viton['jpg'][0].shape)\n",
    "to_pil_image = ToPILImage()\n",
    "img = to_pil_image(sample_viton[\"jpg\"][0].permute(2,0,1))\n",
    "\n",
    "# Display the image\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_viton['ref'][0].shape)\n",
    "to_pil_image = ToPILImage()\n",
    "img = to_pil_image(sample_viton[\"ref\"][0].permute(2,0,1))\n",
    "\n",
    "# Display the image\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new filter for garment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def segment_garment(image_array):\n",
    "    # Get the height and width of the image\n",
    "    height, width, _ = image_array.shape\n",
    "\n",
    "    # Sample the color of the corners, assuming they are part of the background\n",
    "    corner_colors = np.array([\n",
    "        image_array[0, 0],         # Top left corner\n",
    "        image_array[0, width - 1], # Top right corner\n",
    "        image_array[height - 1, 0],       # Bottom left corner\n",
    "        image_array[height - 1, width - 1] # Bottom right corner\n",
    "    ])\n",
    "    \n",
    "    # Compute the average background color\n",
    "    background_color = np.mean(corner_colors, axis=0)\n",
    "\n",
    "    # Compute the Euclidean distance from each pixel to the average background color\n",
    "    color_distance = np.sqrt(((image_array.astype(np.float32) - background_color)**2).sum(axis=2))\n",
    "\n",
    "    # Define a distance threshold\n",
    "    distance_threshold = 30  # Adjust this threshold as needed\n",
    "\n",
    "    # Create a mask where pixels with a distance greater than the threshold are considered foreground\n",
    "    garment_mask = color_distance > distance_threshold\n",
    "\n",
    "    # Convert the boolean mask to a binary mask\n",
    "    garment_mask = (garment_mask * 255).astype(np.uint8)\n",
    "\n",
    "    # Optional: Apply morphological operations to clean up the mask\n",
    "    kernel = np.ones((5,5), np.uint8)\n",
    "    garment_mask = cv2.morphologyEx(garment_mask, cv2.MORPH_OPEN, kernel)\n",
    "    garment_mask = cv2.morphologyEx(garment_mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    return garment_mask\n",
    "\n",
    "# Example usage\n",
    "image_path = '/home/ubuntu/mnt/myvolume/DressCode/lower_body/images/016162_1.jpg'  # Replace with your image path\n",
    "# Load the image using matplotlib\n",
    "image = mpimg.imread(image_path)\n",
    "mask = segment_garment(image)\n",
    "plt.imshow(mask, cmap='gray')  # Show the mask\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(sample_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil_image = ToPILImage()\n",
    "for ref_image in sample[\"ref_image\"]:\n",
    "    img = to_pil_image(ref_image.permute(2,0,1))\n",
    "    img.thumbnail((128,128))\n",
    "    # Display the image\n",
    "    img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mask in sample[\"ref_mask\"]:\n",
    "    plt.imshow(mask, cmap='gray')  # Show the mask\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample for the old DressCode Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[\"cloth\"][0].permute(2,1,0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample[\"model_image\"][0].shape)\n",
    "to_pil_image = ToPILImage()\n",
    "img = to_pil_image(sample[\"model_image\"][0].permute(2,0,1))\n",
    "\n",
    "# Display the image\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil_image = ToPILImage()\n",
    "img = to_pil_image(sample[\"cloth\"][0].permute(2,0,1))\n",
    "\n",
    "# Display the image\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil_image = ToPILImage()\n",
    "img = to_pil_image(sample[\"cloth\"][0])\n",
    "\n",
    "# Display the image\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample[\"model_mask\"][0].shape)\n",
    "plt.imshow(sample[\"model_mask\"][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[\"ref_mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = sample[\"ref_mask\"][0]\n",
    "plt.imshow(mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil_image = ToPILImage()\n",
    "img = to_pil_image(sample[\"m\"][0])\n",
    "\n",
    "# Display the image\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grayscale_transform = Grayscale(num_output_channels=1)\n",
    "gray_tensor = grayscale_transform(sample[\"cloth\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "mask_tensor = gray_tensor < threshold\n",
    "\n",
    "# Convert the boolean mask back to an image for visualization\n",
    "to_pil_image = ToPILImage()\n",
    "mask_image = to_pil_image(mask_tensor.float())\n",
    "\n",
    "# Display the mask\n",
    "mask_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in [\"cloth\"]:\n",
    "    plt.imshow(sample[key][0], interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_image = cv2.imread('/home/ubuntu/mnt/myvolume/DressCode/dresses/images/020716_1.jpg')\n",
    "ref_image = cv2.cvtColor(ref_image, cv2.COLOR_BGR2RGB)\n",
    "ref_mask = (ref_image < 255).astype(np.uint8)[:, :, 0]\n",
    "model_image = cv2.imread('/home/ubuntu/mnt/myvolume/DressCode/dresses/images/020716_0.jpg')\n",
    "model_image = cv2.cvtColor(model_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(model_image)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anydoor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
